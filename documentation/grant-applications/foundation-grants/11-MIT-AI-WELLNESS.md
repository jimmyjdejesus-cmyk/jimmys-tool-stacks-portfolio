# MIT AI and Wellness Seed Grant Application
## AI Safety Tools for Mental Health Protection

**Program**: MIT Schwarzman College of Computing - AI and Wellness Seed Grants  
**Sponsor**: Panasonic Well  
**Amount Requested**: $200,000  
**Duration**: 1 year  
**Website**: https://computing.mit.edu/ai-and-wellness-initiative/

---

## ðŸ“‹ **Pre-Submission Checklist**

- [ ] Review eligibility (industry partners may need MIT faculty collaborator)
- [ ] Identify MIT faculty co-investigator
- [ ] Prepare 3-page project description
- [ ] Prepare budget (max $200K)
- [ ] Check current application deadline

---

## Project Information

### **Project Title**
Protecting Mental Health in the Age of AI: Real-Time Detection of Harmful LLM Outputs

### **Principal Investigator**
[MIT_FACULTY_NAME], MIT [DEPARTMENT]

### **Industry Collaborator**
[PI_NAME], [COMPANY_NAME]

---

## 1. Project Abstract (300 words)

Large Language Models are increasingly used in mental health contextsâ€”therapy chatbots, crisis support, and wellness applications. However, these systems can generate harmful outputs: reinforcing negative self-talk, providing dangerous advice, or responding inappropriately to users in crisis. The mental health implications are severe, particularly for vulnerable populations.

We propose to develop and validate **BiasGuard-Wellness**, a specialized AI safety system that detects potentially harmful LLM outputs in mental health contexts. Building on our existing BiasGuard platform (7-layer semantic pipeline for bias detection), we will create domain-specific detection for:

1. **Crisis mishandling**: Inappropriate responses to users expressing suicidal ideation or self-harm
2. **Harmful advice**: Suggestions that could worsen mental health conditions
3. **Negative reinforcement**: Responses that validate or amplify negative self-perception
4. **Therapeutic boundary violations**: Responses that cross appropriate AI-human boundaries

**Technical Approach**:
We will fine-tune our existing MiniLM-L6-v2 embeddings on mental health corpora, develop pattern libraries for harmful mental health content, and validate against expert-annotated datasets. The system will integrate with therapy chatbot platforms as a safety layer.

**Wellness Impact**:
- Protect vulnerable users from harmful AI outputs
- Enable safer deployment of AI in mental health applications
- Provide tools for mental health app developers to ensure safety
- Contribute to responsible AI development in healthcare

**Deliverables**:
1. BiasGuard-Wellness: Mental health-specific detection module
2. Annotated dataset of harmful vs. safe mental health AI outputs
3. Integration guidelines for therapy chatbot platforms
4. Peer-reviewed publication on findings

This work directly addresses the AI and wellness intersection by protecting mental health while enabling beneficial AI applications.